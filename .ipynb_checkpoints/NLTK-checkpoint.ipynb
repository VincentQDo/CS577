{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Intro to NLTK\n\nIn 2001, NLTK was created as part of a computational linguistics course at UPenn. Today, it's open source. It simplifies common language processing tasks into a framework.\n\nOther natural language frameworks do exist today. NLTK algorithms may not be as advanced or highly optimizes as what is found in other toolkits.\n\nNLTK defines a basic infrastructure that can be used to build NLP programs in Python. It provides: \n* Basic classes for representing data relevant to natural language processing \n* Standard interfaces for performing tasks, such as tokenization, tagging, and parsing \n* Demonstrations (parsers, chunkers, chatbots) \n* Extensive documentation, including tutorials and reference documentation "
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Installing NLTK\n\nWe first need to install NLTK onto our system. Once it's installed, the following import should work.\n\nSee http://www.nltk.org/install.html. There's a: Windows binary installation and Mac/Unix command-line installation.\n\nOn my Mac, I used the command `pip3 install nltk`."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import nltk",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "This notebook follows Chapter 1 in the NLTK book, which explores some basic text processing on books that are pre-loaded into NLTK. \n\nThe default NLTK installation is only the bare minimum. Other parts of the toolkit can be installed such as \ncorpora, taggers, parsers, etc. \n\nThe NLTK book collection has to be first downloaded onto your system. <u>Download the book collection.</u> This only has to be done once."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "nltk.download()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Loading the `book` collection\n\nFrom NLTK's `book` module, load all items."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from nltk.book import *",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### More information about a specific NLTK text"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "text1",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "text2",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Searching a text: `concordance`\n\nShows every occurrence of a given word, together with some context."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "text1.concordance('monstrous')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "-"
        }
      },
      "cell_type": "markdown",
      "source": "### Dispersion Plot\n\nCan be used to investigate changes in language use over time:\n* each stripe represents an instance of a word \n* each row represents the entire text \n\nIn this example, the text4 book is an artificial text constructed by joining the texts of the Inaugural Address Corpus end-to-end. **What do you expect this graph to look like?**\n\n(In order for this to work, the Python library `matplotlib` also needs to be installed. On my Mac, I had to do `pip3 install matplotlib`.)"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "%matplotlib notebook\n\ntext4.dispersion_plot([\"citizens\",\"democracy\",\"freedom\",\"duties\",\"America\"])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Counting Tokens\n\nCalculating length of a text from start to finish."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "len(text3)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "_What does this mean?_\n\nThe book of Genesis has 44,764 **tokens**.\n* sequence of characters that are treated as a group\n* usually words and punctuation symbols (no spaces)"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Vocabulary\n\nVocabulary of a text is the set of tokens that it uses. (Recall that there are no duplicates in a set.) \n\nHow many **distinct words** does the book of Genesis contain? "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "sorted(set(text3))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "len(set(text3))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Note that the number of unique types includes punctuation symbols, so it’s not completely accurately to say that there are 2,789 different words. "
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Lexical Diversity (TTR: Type-Token Radio)\n\nEquivalent to a measure of lexical richness.\n\n$$ Lexical Diversity = \\frac{Text Length}{Number of Unique Types} $$"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def lexical_diversity(text):\n    return len(text) / len(set(text))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "lexical_diversity(text3)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "lexical_diversity(text5)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Necessity for writers to re-use several function words, so lexical diversity is better used for comparing texts of equal length."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Lexical Diversity in the Brown Corpus\n\n**Corpus:** a collection of “real-world” text (plural is corpora)\n\nBrown Corpus - famous corpus compiled in the 1960s at Brown University. \n* a general corpus of 500 samples of English-language text, totaling approximately one million words, compiled from works published in the United States in 1961 \n\n<u>Lexical Diversity of various genres in the Brown corpus:</u>\n\n```\nGenre              Tokens    Types   Lexical Diversity\nSkill and Hobbies  82345     11935   6.9\nHumor              21695     5017    4.3\nFiction: science   14470     3233    4.5\nPress: reportage   100554    14394   7.0\nFiction: romance   70022     8452    8.3\nReligion           39399     6373    6.2\n```"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Frequency Distributions\n\nA frequency distribution contains the frequency of each vocabulary item in the text. \n\nIn other words, it contains the count of every word. \n\n_What Python data structure would be used to represent a Frequency Distribution?_"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### NLTK's `FreqDist`\n\nNLTK has built-in support for maintaining the data in a frequency distribution. \n\n* If you look at the NLTK code (since it’s open-source), all it is doing is using a Python dictionary in the background. \n\nFinding the 50 most frequent words of Moby Dick:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "fdist1 = FreqDist(text1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(fdist1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "fdist1.most_common(50)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "fdist1['whale']",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "type(fdist1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Common Words and Hapaxes\n\nNotice anything?"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "text1",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "fdist1.most_common(50)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Notice that the most frequent words of Moby Dick don’t describe the topic of genre of the text. This is a common finding! \n* whale is the exception\n* **hapaxes** (plural of “hapax”) are the words that only appear once in a language, or written work \n  * Sometimes hapaxes help with their context"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "fdist1.hapaxes()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Fine-Grained Selection of Words\n\nList comprehensions will come in very handy.\n\nLet's build a list comprehension that finds _frequently occurring long words_.\n\nGeneral syntax: `[w for w in V if p(w)]`\n* iterates over a collection and returns a list \n* (remember: duplicates are possible in a list) \n* read as: “for each word w in collection V, if p(w) is true, then add w to the returned list” "
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Find words from the vocabulary of a text that are more than 15 characters long. "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "V = set(text1)\nlong_words = [w for w in V if len(w) > 15]\nsorted(long_words)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Find commonly occurring long words in a text."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "fdist5 = FreqDist(text5)\nsorted([w for w in set(text5) if len(w) > 7 and fdist5[w] > 7])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Find the most frequent word length in a text."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "[len(w) for w in text1]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "fdist = FreqDist(len(w) for w in text1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(fdist)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "fdist",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "fdist.most_common()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "fdist.max()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "fdist[3]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "fdist.freq(3)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Functions defined for NLTK's `FreqDist`\n\n```\nExample                           Description\n-------                           -----------\nfdist = FreqDist(samples)         create a frequency distribution containing the given samples\nfdist[sample] += 1                increment the count for this sample\nfdist['monstrous']                count of the number of times a given sample occurred\nfdist.freq('monstrous')           frequency of a given sample\nfdist.N()                         total number of samples\nfdist.most_common(n)              the n most common samples and their frequencies\nfor sample in fdist:              iterate over the samples\nfdist.max()                       sample with the greatest count\nfdist.tabulate()                  tabulate the frequency distribution\nfdist.plot()                      graphical plot of the frequency distribution\nfdist.plot(cumulative=True)       cumulative plot of the frequency distribution\nfdist1 |= fdist2                  update fdist1 with counts from fdist2\nfdist1 < fdist2                   test if samples in fdist1 occur less frequently than in fdist2\n```"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Example: Counting Word Occurrences <u>without</u> `FreqDist`"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "nltk.corpus.gutenberg.words('shakespeare-macbeth.txt')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "count = {}\nfor word in nltk.corpus.gutenberg.words('shakespeare-macbeth.txt'):\n    word = word.lower()\n    if word not in count:\n        count[word]=0\n    count[word] += 1",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now inspect the dictionary:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "count['scotland']",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "frequencies = [(freq,word) for (word,freq) in count.items()]\nfrequencies.sort()\nfrequencies.reverse()\nfrequencies[:20]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Alternatives to NLTK\n\n* http://en.wikipedia.org/wiki/Outline_of_natural_language_processing#Natural_language_processing_toolkits\n* StanfordNLP, LingPipe, Mallet are popular\n  * for Java, not Python "
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}